{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---\n",
    "\n",
    "## Technical Report - Notebook 5\n",
    "\n",
    "---\n",
    "\n",
    "# Can Social Data Predict Natural Catastrophes?\n",
    "\n",
    "_An investigation into whether twitter data can predict severe natural events that cause major economic loss._\n",
    "\n",
    "<img src=\"images/brisbane_hail_27112.jpg\",width=12000,height=10000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Contents\n",
    "\n",
    "1. Introduction: Problem Statement and \t\t\t\t\n",
    "2. Data Aquisition and Querying\t\t\t\t\n",
    "3. Data Parsing: Identifying outliers, defining variables, cleaning data\n",
    "    - Data pipelines - importing/sorting/querying/munging\t\t\n",
    "4. Statistical Analysis: Data correlation and visualisation\t\t\t\t\t\t\n",
    "6. Model: Perform model (train subset as needed)\t\t\t\t\n",
    "7. Model: Tune and evaluate model\t\t\t\t\t\t\n",
    "10. Algorithm Development: Discuss model selection and implementation process\t\t\t\t\n",
    "    - Model performance, tuning and \n",
    "11. Project Success: Interpretation of findings and goals review\t\t\n",
    "12. Model Deployment: Retraining and deploying model in a production environment \n",
    "13. Stakeholder Recommendation and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executive Summary\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "\n",
    "The overall goal of this project is to assess whether twitter data can predict a natural catastrophe.\n",
    "\n",
    "The key clients who will be interested in this type of analysis are insurance companies. The Insurance Council of Australia (ICA) classifies a catastrophe as:\"large natural or man-made disasters that cause a significant number of claims in a region.\" More specifically this refers to:\n",
    "- Significant financial/insurance loss (> 10M AUD)\n",
    "- Significant number of claims or damage reports\n",
    "- Wide reported geographic extent\n",
    "\n",
    "As it was not possible to analyse all catastrophe types within the timeframe of the investigation, I will focus on analysing Twitter data for two major Australian hail storm events:\n",
    " 1. Brisbane Hail Storm (27th November 2014): AUD 1,400m insurance loss\n",
    " 2. Sydney Hail Storm (25th April 2015): AUD 400m insurance loss\n",
    "\n",
    "These events were chosen because they are two of the most significant hail storms (from an insurance perspective) during the Twitter age, but their catastrophic loss attributes (loss, claims and geography) are significantly different.\n",
    "\n",
    "The ultimate goal of this project is to answer two key questions:\n",
    "\n",
    "    - Can twitter data detect hail events from the language people use in tweets?\n",
    "    - Can twitter data identify severe catastrophic hail events?\n",
    "\n",
    "Identify language in tweets that confirms a specific type of catastrophe is occuring (i.e.: hail),  major damage to property or infrastructure and analysing sentiment are critical components of this analysis to develop a model that can effectively classify twitter data.\n",
    "\n",
    "The following report will outline the methods applied in my investigation and the results of the algorithm development. I will finish with a review of the successes and how this model could be deployed in real time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Acquisition & Querying\n",
    "\n",
    "I sourced my main datasets from Sifter. Sifter is a service that provides search and retrieve access to every undeleted Tweet in the history of Twitter.\n",
    "\n",
    "https://sifter.texifter.com/\n",
    "\n",
    "For my project I purchased two datasets:\n",
    "\n",
    "- Primary dataset: Brisbane Hail storm on 27th November 2015 With the help of sifter, I performed the following search function to extract the data:\n",
    "\n",
    "_Rule: (contains hail OR storm OR damage OR flood OR insur OR \"golf ball\"~6 OR \"tennis ball\"~6 OR lightning OR thunder OR #brisbanehail OR #brisbanestorm OR \"brisbane hail\"~6 OR \"brisbane storm\"~6 OR #australiahail OR #australiastorm OR \"australia hail\"~6 OR \"australia storm\"~6 OR #auhail OR #austorm OR \"au hail\"~6 OR \"au storm\"~6 OR #qldhail OR #qldstorm OR \"qld hail\"~6 OR \"qld storm\"~6 OR #queenslandhail OR #queenslandstorm OR \"queensland hail\"~6 OR \"queensland storm\"~6 -(brisbane OR qld OR queensland OR australia OR au OR seqld) All duplicates removed._\n",
    "\n",
    "This query aimed to capture all potential tweets that could relate to the identified hail event on 27th November 2014. I extracted 3 days worth of data however I will only analyse a 24 hour period surrounding the known time of the hail storm (15:00 - 17:00 AEDT), as I am primarily interested in the 'real-time' twitter indicators. This event was more widespread and had a higher insured loss, I will use this as my primary dataset for the EDA/model building (i.e.: my training data). The final dataset was delivered in csv format.\n",
    "\n",
    "- Secondary dataset: Sydney Hail storm I performed a similar search in Sifter to gather data for a hail event in Sydney on 25th April 2015.\n",
    "\n",
    "_Rule: (contains hail OR storm OR damage OR flood OR \"golf ball\"~6 OR \"tennis ball\"~6 OR insur OR lightning OR thunder OR #sydneyhail OR #sydneystorm OR \"sydney hail\"~6 OR \"sydney storm\"~6 OR #australiahail OR #australiastorm OR \"australia hail\"~6 OR \" australia storm\"~6 OR #auhail OR #austorm OR \"au hail\"~6 OR \"au storm\"~6 OR #nswhail OR #nswstorm OR \"nsw hail\"~6 OR \"nsw storm\"~6 -(sydney OR nsw OR \"new south wales\" OR australia OR au) All duplicates removed._\n",
    "\n",
    "This dataset was used as my secondary data . The final dataset was delivered in csv format.\n",
    "Supporting dataset: ICA Catastrophe Data 2016 This dataset was sourced from the Insurance Council of Australia (ICA). The Insurance Council of Australia collects catastrophe related claims data from the Australian market as part of its role in supporting the industry to deliver repairs, rebuilding and recovery services following large disasters. The ICA Catastrophe Database commenced in 1967 and records insurance loss estimates for declared insurance catastrophe events.\n",
    "\n",
    "This dataset will be used as a reference for the two major hail events in 2014 and 2015. It will also serve as a useful guide for future events to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 3. Data Parsing: Identifying outliers, defining variables, cleaning data\n",
    "\n",
    "This section of the report outlines how data was prepared for modelling and includes critical feedback on my approach. I have found that analysing text data can be challenging, particularly Twitter data(!), but if NLP is performed correctly the results can be powerful.\n",
    "\n",
    "The preparation of data can be broken down into two main processes, or pipelines:\n",
    "\n",
    "1. Twitter User Data Preparation: Using the atrributes associated with each tweet to remove irrelevant columns, anomalous records and data outliers. This refines the tweet records to increase the confidence of the data.\n",
    "2. Tweet Text Data Preparation: Using NLP processes to clean the text data and create a vectorised dataset of key words. This data is what we will use in modelling.\n",
    "\n",
    "**Twitter User Data Preparation:**\n",
    "\n",
    "The priority of this exercise was to refine the dataset and ensure the tweets that remained are of good quality. A number of issues were identified during this exercise, the first of which was \n",
    "\n",
    "\n",
    "**_Word Exploratory Data Analysis:_**\n",
    "\n",
    "Prior to analysing key words that appear in the sample tweets, I performed a series of natural langu\n",
    "\n",
    "\n",
    "Using tokenisation, removing stop words and other natural language processing techniques is the next step to create the critical data I will use for modelling; a vectorised dataframe of all the key words used in tweets. Vectorisation also allows me to easily create targets for classification (i.e.: using the 'hail' word use as a class).\n",
    "\n",
    "\n",
    "\n",
    "The statistical analysis of this data has shown that within our sample, there is clearly a skew of word counts to a low frequency, however there are some key words which are prevalent throughout the tweets with similar high counts due to the volume of tweets on that subject (i.e.: the hail events). Comparing the words that occur during the two events, especially over time, has been crucial in supporting my idea that twitter can identify natural catastrophes... So much so that another type of event - Nepal Earthquake - was observed in my EDA.\n",
    "\n",
    "**_LDA - Topic Modelling_**\n",
    "\n",
    "This stage of the analysis has helped identify the core words within the core hail 'topic', as well as other major topics that were tweeted. Grouping the data (and subsequent topic modelling) is also another way of powerfully analysing the key words that occur together; by narrowing the dataset into regions which are specific to the hail events, the key words (and potentially predictors) indicating hail are more strongly recognised.\n",
    "\n",
    "**_Modelling_**\n",
    "\n",
    "Whilst modelling is still in the preliminary investigation phase, I have identified that classification algorithms will be the most appropriate for my text analysis. Initially I have investigated logisitic regression and decision tree classifications. I have selected key words that indicate hail storms, as observed during BOW analysis and topic modelling, for my initial predictors and generated a hail 'class' as the target variable.\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "### - Next Steps -\n",
    "\n",
    "1. More in-depth feature selection and principle component analysis.\n",
    "- Perform training/testing of Brisbane Event vs. Sydney Event.\n",
    "- Investigation of other classification models (SVM, Naive Bayes and Random Forest in particular)\n",
    "- (Potentially) A sentiment analysis of the twitter data: Can attitudes of tweets help classify a natural catastrophe?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Statsitcal Analysis: Relationships, Visualisations and NLP Results\n",
    "\n",
    "#### User Data Observations:\n",
    "\n",
    "\n",
    "\n",
    "#### Tweet Data Observations:\n",
    "\n",
    "_Bag-Of-Words Analysis:_\n",
    "\n",
    "This has identified key words within the twitter text for the whole dataset and for our two independent events. It also indentified further data to exclude (i.e.: the term 'gemini') and we reviewed the most common words used with our key word 'hail'. This analysis also helped identify other key events, which indicates the strength of the natural language processing techniques used.\n",
    "\n",
    "_Topic Modelling:_\n",
    "\n",
    "The LDA has successfully identified key groups of terms in our data, particularly for the hail events. This has strengthened our understanding of the key words that are used by twitter users during an event and helped identify other topics which may cause certain terms to deviate away from our expected classifications. Overall though, identifying other major events in the data (thanksgiving, boko haram, NRL matches, Nepal Earthquake) and being able to relate these to known events is a valuable exercise and could certainly help when developing a model to categorise words in tweets for future models.\n",
    "\n",
    "_Sentiment Analysis:_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Successes, Setbacks, & Lessons Learned -\n",
    "\n",
    "**Successes:** The main successes in my project so far include:\n",
    "- Using the user information to effectively remove irrelevant records from my dataset. This include large amounts of data from automated tweeting services such as Twittascope and removing outliers appropriate to their distributions.\n",
    "- NLP: Cleaning the tweet text data, and the resulting most-frequent terms, has been a big success so far. In addition to successfully identifying words that relate to the hail events in question, signatures of other major natural catastrophes are also showing up.\n",
    "- The topic modelling also reinforces the categorisation potential of tweet text, particularly concerning the hail events, and indicates what other events may produce words that are used in hail tweets.\n",
    "- And overalll... Generating a dataset from tweets that can use any word, or sets of words, as a class in classification modeling has been a crucial process.\n",
    "\n",
    "**Setbacks:** Several setbacks have been encountered with the project so far:\n",
    "- One setback was finding additional records to be removed during the tweet text analysis. This added a further cleaning component to refine our dataset.\n",
    "- The topic modelling could be refined to in/decrease the topic numbers and group sizes. This would reduce noise in some of the groups with less confident word selections. It must also be noted that on the dates of the samples, tweets had a maximum size limit of 140 words. This means that forcing larger numbers of words in LDA will easily produce noise in topics from neighbouring tweets, which is why so many non-hail topics had meteorological words within them. So for tweets - the smaller the groups, the better!\n",
    "- Whilst the initial results of our model are positive, there are obvious setbacks in the feature selection processes. A principle component analysis (PCA) on a wider set of predictors must be performed as a next step prior to modelling. In the next stage of my capstone I will perform a more robust PCA and feature selection prior to modelling to try and improve the accuracy and confusion matricies. _I acknowledge that the few predictors used in the preliminary modelling are not enough for this type of classification modelling._\n",
    "\n",
    "**Lessons Learned:**\n",
    "- Dealing with null values and removing outliers appropriately is important however, there is a trade-off between improving the distribution of a variable and knocking out too much data.\n",
    "- The natural language processes and the sequence in which they are performed are very important when analysing text data, particularly 'messy' data such as tweets.\n",
    "- The method of using key terms from Bag-Of-Words and topic modelling is useful for textual analysis and identifying common terms, however limiting the predictors in classification modelling using these results is likely inhibiting the success of the algorithms. Despite the high accuracy of our models, feature selection must be more robust to improve model performance.\n",
    "- Other algorithms may be more appropriate for this project and further investigation will be needed to ensure the correct machine learning methods are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
